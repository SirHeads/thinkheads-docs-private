---
Project: Learning Assistant
Type: Specifications
Date: 2025-07-17
Spec-ID: SPEC-LA-BACK
---

# Learning Assistant V0 Backend Specifications

## Overview
This document details the technical specifications for the backend of the Learning Assistant V0, corresponding to **SPEC-LA-BACK** from **4.2.2 Learning_Assistant_Specifications_Overview.md**. The backend provides API endpoints for LLM-driven resource suggestions and reinforcement learning exercises, supporting requirements FR01 (resource suggestions), FR03 (reinforcement learning exercises), and NFR04 (performance). It is designed for solo development within a 20-hour/week schedule, using Python/FastAPI on Linode for lightweight APIs and Ollama on Proxmox (LXC container `dockProd1`, 1 RTX 5060 Ti GPU, 32 GB RAM) for LLM processing, integrated with the AI-driven website.

## Technical Specifications
- **Technology**: Python/FastAPI for API endpoints, Ollama for LLM processing.
- **Hosting**:
  - FastAPI: Docker container on Linode (4 GB RAM, 2 CPU cores, 80 GB storage, Debian OS).
  - Ollama: LXC container `dockProd1` on Proxmox, allocated 1 RTX 5060 Ti GPU, 32 GB RAM.
- **Configuration**:
  - FastAPI server (`main.py`):
    ```python
    from fastapi import FastAPI
    from pydantic import BaseModel
    import ollama

    app = FastAPI()

    class ResourceRequest(BaseModel):
        query: str
        session_id: str

    class ExerciseRequest(BaseModel):
        exercise_type: str
        session_id: str

    @app.post("/api/learning/resources")
    async def get_resources(request: ResourceRequest):
        response = ollama.chat(
            model="llama3",
            messages=[{"role": "user", "content": f"Find AI/ML resources for: {request.query}"}],
            context={"session_id": request.session_id}
        )
        return {"resources": response["message"]["content"]}

    @app.post("/api/learning/exercises")
    async def get_exercises(request: ExerciseRequest):
        response = ollama.chat(
            model="llama3",
            messages=[{"role": "user", "content": f"Generate {request.exercise_type} exercise"}],
            context={"session_id": request.session_id}
        )
        return {"exercise": response["message"]["content"]}
    ```
  - Ollama: Configured with fine-tuned `llama3` model, RAG pipeline using PostgreSQL embeddings for context-aware responses.
  - API endpoints proxied via Nginx (`/api/learning/`), secured by Cloudflare Tunnel.
- **Features**:
  - Resource suggestion endpoint (`/api/learning/resources`) returns 3+ relevant AI/ML resources (e.g., papers, tutorials) based on user queries (FR01).
  - Exercise endpoint (`/api/learning/exercises`) generates interactive exercises (e.g., Q&A, coding tasks) with feedback (FR03).
  - Session management links queries to user sessions in PostgreSQL (NFR04).
- **Performance**: LLM responses <2s for 95% of queries, supporting 10 concurrent users (NFR04).

## Implementation Details
- **Development**: Code FastAPI in VS Code, push to Git on Proxmox, deploy via Docker on Linode.
- **RAG Pipeline**: Markdown documents and curated resources parsed by Python script, embeddings generated by Ollama, stored in PostgreSQL (`rag_embeddings` table).
  - Example query: `SELECT document_path FROM rag_embeddings ORDER BY embedding <=> ollama_embed(query) LIMIT 3`.
- **Testing**: Test Ollama in `dockTest1` (Proxmox) for response accuracy, validate APIs on Linode for latency and concurrency.
- **Deployment**: FastAPI deployed via `docker-compose` on Linode; Ollama in `dockProd1` with GPU passthrough, updated via n8n.
- **Dependencies**: Requires AI-driven website (Nginx, Cloudflare) for integration, PostgreSQL (`dockProd2`) for data storage.

## RAG Integration
- **Usage**: Feeds into Meeting Roomâ€™s RAG pipeline for development planning.
- **Structure**: Consistent headers, code snippets, and metadata for parseability.
- **Storage**: Store in `/docs/projects/learning_assistant_specifications/`, sync to Linode, embed in PostgreSQL for RAG queries.