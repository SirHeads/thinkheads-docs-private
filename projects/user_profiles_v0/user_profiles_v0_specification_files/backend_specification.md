---
Project: User Profiles
Type: Specifications
Date: 2025-07-17
Spec-ID: SPEC-UP-BACK
---

# User Profiles V0 Backend Specifications

## Overview
This document details the technical specifications for the backend of the User Profiles V0, corresponding to **SPEC-UP-BACK** from **4.5.2 User_Profiles_Specifications_Overview.md**. The backend provides API endpoints for user profile management and LLM-driven personalization, supporting requirements FR01 (profile creation), FR02 (learning progress tracking), and NFR04 (performance). It is designed for solo development within a 20-hour/week schedule, using Python/FastAPI on Linode for lightweight APIs and Ollama on Proxmox (LXC container `dockProd1`, 1 RTX 5060 Ti GPU, 32 GB RAM) for LLM processing, integrated with the AI-driven website.

## Technical Specifications
- **Technology**: Python/FastAPI for API endpoints, Ollama for LLM-driven personalization.
- **Hosting**:
  - FastAPI: Docker container on Linode (4 GB RAM, 2 CPU cores, 80 GB storage, Debian OS).
  - Ollama: LXC container `dockProd1` on Proxmox, allocated 1 RTX 5060 Ti GPU, 32 GB RAM.
- **Configuration**:
  - FastAPI server (`main.py`):
    ```python
    from fastapi import FastAPI
    from pydantic import BaseModel
    import ollama
    from fastapi_users import FastAPIUsers
    from fastapi_users.authentication import CookieTransport, JWTStrategy
    from fastapi_users_db_sqlalchemy import SQLAlchemyUserDatabase
    from sqlalchemy.ext.asyncio import AsyncSession

    app = FastAPI()
    cookie_transport = CookieTransport(cookie_max_age=3600)
    SECRET = "your-secret-key"
    
    def get_jwt_strategy() -> JWTStrategy:
        return JWTStrategy(secret=SECRET, lifetime_seconds=3600)
    
    fastapi_users = FastAPIUsers[SQLAlchemyUserDatabase, str](
        user_db=SQLAlchemyUserDatabase(AsyncSession, user_table),
        auth_backends=[get_jwt_strategy()]
    )

    class ProfileRequest(BaseModel):
        email: str
        password: str
        username: str

    class ProgressRequest(BaseModel):
        user_id: str
        completed_task: str

    @app.post("/api/profiles/create")
    async def create_profile(request: ProfileRequest):
        user = await fastapi_users.create_user(
            email=request.email,
            password=request.password,
            username=request.username
        )
        return {"user_id": str(user.id), "status": "created"}

    @app.post("/api/profiles/progress")
    async def track_progress(request: ProgressRequest):
        response = ollama.chat(
            model="llama3",
            messages=[{"role": "user", "content": f"Analyze progress for task: {request.completed_task}"}],
            context={"user_id": request.user_id}
        )
        return {"progress": response["message"]["content"]}
    ```
  - Ollama: Configured with fine-tuned `llama3` model, RAG pipeline using PostgreSQL embeddings for context-aware progress insights.
  - API endpoints proxied via Nginx (`/api/profiles/`), secured by Cloudflare Tunnel.
- **Features**:
  - Profile creation endpoint (`/api/profiles/create`) supports email/password and OAuth (e.g., GitHub) authentication (FR01).
  - Progress tracking endpoint (`/api/profiles/progress`) provides AI-driven insights on completed tasks, integrated with Learning Assistant data (FR02).
  - Session management links profiles to user sessions in PostgreSQL (NFR04).
- **Performance**: LLM responses <2s for 95% of queries, supporting 100 concurrent users (NFR04).

## Implementation Details
- **Development**: Code FastAPI in VS Code, push to Git on Proxmox, deploy via Docker on Linode.
- **RAG Pipeline**: User progress data and markdown documents parsed by Python script, embeddings generated by Ollama, stored in PostgreSQL (`rag_embeddings` table).
  - Example query: `SELECT document_path FROM rag_embeddings ORDER BY embedding <=> ollama_embed(progress_query) LIMIT 3`.
- **Testing**: Test Ollama in `dockTest1` (Proxmox) for response accuracy, validate APIs on Linode for latency and concurrency.
- **Deployment**: FastAPI deployed via `docker-compose` on Linode; Ollama in `dockProd1` with GPU passthrough, updated via n8n.
- **Dependencies**: Requires AI-driven website (Nginx, Cloudflare) for integration, PostgreSQL (`dockProd2`) for data storage, FastAPI Users for authentication.

## RAG Integration
- **Usage**: Feeds into Meeting Roomâ€™s RAG pipeline for planning and personalization.
- **Structure**: Consistent headers, code snippets, and metadata for parseability.
- **Storage**: Store in `/docs/projects/user_profiles_specifications/`, sync to Linode, embed in PostgreSQL for RAG queries.