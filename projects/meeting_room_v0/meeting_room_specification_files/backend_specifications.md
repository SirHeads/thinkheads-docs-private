---
Project: Meeting Room
Type: Specifications
Date: 2025-07-17
Spec-ID: SPEC-MR-BACK
---

# Meeting Room V0 Backend Specifications

## Overview
This document details the technical specifications for the backend of the Meeting Room V0, corresponding to **SPEC-MR-BACK** from **4.3.2 Meeting_Room_Specifications_Overview.md**. The backend provides API endpoints for LLM-driven agenda generation and meeting insights, supporting requirements FR01 (agenda generation), FR03 (meeting insights), and NFR04 (performance). It is designed for solo development within a 20-hour/week schedule, using Python/FastAPI on Linode for lightweight APIs and Ollama on Proxmox (LXC container `dockProd1`, 1 RTX 5060 Ti GPU, 32 GB RAM) for LLM processing, integrated with the AI-driven website.

## Technical Specifications
- **Technology**: Python/FastAPI for API endpoints, Ollama for LLM processing.
- **Hosting**:
  - FastAPI: Docker container on Linode (4 GB RAM, 2 CPU cores, 80 GB storage, Debian OS).
  - Ollama: LXC container `dockProd1` on Proxmox, allocated 1 RTX 5060 Ti GPU, 32 GB RAM.
- **Configuration**:
  - FastAPI server (`main.py`):
    ```python
    from fastapi import FastAPI
    from pydantic import BaseModel
    import ollama

    app = FastAPI()

    class AgendaRequest(BaseModel):
        meeting_goal: str
        session_id: str

    class InsightRequest(BaseModel):
        meeting_notes: str
        session_id: str

    @app.post("/api/meeting/agenda")
    async def generate_agenda(request: AgendaRequest):
        response = ollama.chat(
            model="llama3",
            messages=[{"role": "user", "content": f"Generate a meeting agenda for: {request.meeting_goal}"}],
            context={"session_id": request.session_id}
        )
        return {"agenda": response["message"]["content"]}

    @app.post("/api/meeting/insights")
    async def generate_insights(request: InsightRequest):
        response = ollama.chat(
            model="llama3",
            messages=[{"role": "user", "content": f"Summarize and provide insights for: {request.meeting_notes}"}],
            context={"session_id": request.session_id}
        )
        return {"insights": response["message"]["content"]}
    ```
  - Ollama: Configured with fine-tuned `llama3` model, RAG pipeline using PostgreSQL embeddings for context-aware responses.
  - API endpoints proxied via Nginx (`/api/meeting/`), secured by Cloudflare Tunnel.
- **Features**:
  - Agenda generation endpoint (`/api/meeting/agenda`) creates structured agendas with 3+ actionable items based on user input (FR01).
  - Insights endpoint (`/api目を

System: /api/meeting/insights`) generates summaries and key insights from meeting notes (FR03).
  - Session management links queries to user sessions in PostgreSQL (NFR04).
- **Performance**: LLM responses <2s for 95% of queries, supporting 10 concurrent users (NFR04).

## Implementation Details
- **Development**: Code FastAPI in VS Code, push to Git on Proxmox, deploy via Docker on Linode.
- **RAG Pipeline**: Meeting notes and markdown documents parsed by Python script, embeddings generated by Ollama, stored in PostgreSQL (`rag_embeddings` table).
  - Example query: `SELECT document_path FROM rag_embeddings ORDER BY embedding <=> ollama_embed(query) LIMIT 3`.
- **Testing**: Test Ollama in `dockTest1` (Proxmox) for response accuracy, validate APIs on Linode for latency and concurrency.
- **Deployment**: FastAPI deployed via `docker-compose` on Linode; Ollama in `dockProd1` with GPU passthrough, updated via n8n.
- **Dependencies**: Requires AI-driven website (Nginx, Cloudflare) for integration, PostgreSQL (`dockProd2`) for data storage.
- **RAG Integration**: Store in `/docs/projects/meeting_room_specifications/`, sync to Linode, embed in PostgreSQL for RAG queries.